{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import webbrowser \n",
    "import textwrap\n",
    "import math\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install a pretrained sentence transformers model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories for scraped and processed texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_texts_dir = 'scraped_texts'\n",
    "processed_texts_dir = 'processed_texts'\n",
    "os.makedirs(scraped_texts_dir, exist_ok=True)\n",
    "os.makedirs(processed_texts_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the preprint containers (with the preprint title, link, author names) from the search page\n",
    "def read_search_page(url):\n",
    "    uClient = uReq(url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'html.parser')\n",
    "    containers = page_soup.findAll('div', {'class':'highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix'})\n",
    "    return containers\n",
    "\n",
    "# Returns the number of results from the search page\n",
    "def get_num_results():\n",
    "    search_term = searchfield.get()\n",
    "    search_term = search_term.split(' ')\n",
    "    search_term = '%252B'.join(search_term)\n",
    "    site = sites.get()\n",
    "    first_part = 'https://www.' + site + '.org/search/'\n",
    "    search_url = first_part + search_term + '%20numresults%3A10%20sort%3Apublication-date%20direction%3Adescending' \n",
    "    ########################################################################################\n",
    "    #search_url = 'some other url here' #if you would like to input a full custom search url\n",
    "    ########################################################################################\n",
    "    uClient = uReq(search_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'html.parser')\n",
    "    \n",
    "    num = re.findall(r'\\d+',page_soup.h1.text.strip())\n",
    "    if num:\n",
    "        num_results = int(''.join(num))\n",
    "        l3_string.set(str(num_results) + ' results found. Retrieve papers? Sorted by newest to oldest.')   \n",
    "        max_results = []\n",
    "        for pages in range(math.ceil(num_results/10)):\n",
    "            if (pages+1) == (math.ceil(num_results/10)):\n",
    "                max_results.append(str((pages)*10+num_results % 10) + ' max results')\n",
    "            else:\n",
    "                max_results.append(str((pages+1)*10) + ' max results')\n",
    "\n",
    "        maxresults['values'] = max_results\n",
    "        maxresults.grid(row = 1, column = 6, sticky = E, pady = 2) \n",
    "        collectbtn.grid(row = 1, column = 7, sticky = E, pady = 2)\n",
    "        \n",
    "    else:\n",
    "        l3_string.set('No results found')\n",
    "        maxresults.grid_forget()\n",
    "        collectbtn.grid_forget()\n",
    "    return num_results\n",
    "\n",
    "# Returns text from a preprint page\n",
    "def read_article_page(url): \n",
    "    uClient = uReq(url+'.full')\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'html.parser')\n",
    "\n",
    "    date = page_soup.findAll('div',{'class':'panel-pane pane-custom pane-1'})[0].div.text.strip()\n",
    "    date = re.sub('\\.','',date[7:])\n",
    "    \n",
    "    article_soup = page_soup.find_all('div',{'class':re.compile('^article ')})[0]\n",
    "\n",
    "    for toremove in article_soup.find_all(id=[re.compile('^F\\d'),(re.compile('^T\\d'))]):\n",
    "        toremove.decompose()\n",
    "    for toremove in article_soup.find_all('a',{'class':'xref-bibr'}):\n",
    "        toremove.decompose()\n",
    "    for toremove in article_soup.find_all('a',{'class':'xref-fig'}):\n",
    "        toremove.decompose()\n",
    "    for toremove in article_soup.find_all('sup'):\n",
    "        toremove.decompose()\n",
    "\n",
    "    main_titles = []\n",
    "    subtitles = []\n",
    "    curr_section = []\n",
    "\n",
    "    abstract = []\n",
    "    introduction = []\n",
    "    results = []\n",
    "    methods = []\n",
    "    discussion = []\n",
    "    \n",
    "    intro = False\n",
    "    for title in article_soup.find_all('h2'):\n",
    "        if ('introduction' in (title.text).lower()) | ('main' in (title.text).lower()):\n",
    "            intro = True # for dealing with labeling inconsistencies for introduction sections\n",
    "\n",
    "    for tag in article_soup.find_all(True):\n",
    "        if (re.match('^h', tag.name)):\n",
    "            if tag.name == 'h2':\n",
    "                main_titles.append(tag.text.lower())\n",
    "                curr_section = tag.text.lower()\n",
    "            else:\n",
    "                subtitles.append(tag.text.lower())\n",
    "        if (re.match('^p', tag.name)):\n",
    "            if ('abstract' in curr_section) | ('summary' in curr_section):\n",
    "                if (intro == False) & (len(abstract)>0):  # sometimes abstracts go straight into introductions\n",
    "                    introduction += [tag.text.strip()]\n",
    "                else:\n",
    "                    abstract += [tag.text.strip()]\n",
    "            if ('introduction' in curr_section) | ('main' in curr_section):\n",
    "                introduction += [tag.text.strip()]\n",
    "            if 'results' in curr_section:\n",
    "                results += [tag.text.strip()]\n",
    "            if 'methods' in curr_section:\n",
    "                methods += [tag.text.strip()]\n",
    "            if 'discussion' in curr_section:\n",
    "                discussion += [tag.text.strip()]        \n",
    "    return abstract, introduction, results, methods, discussion, date\n",
    "\n",
    "# Goes into each container from the search page and gets text from each preprint page\n",
    "def get_papers(search_url, max_pages):\n",
    "    uClient = uReq(search_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'html.parser')   \n",
    "    num_pages = max_pages\n",
    "    site = sites.get()\n",
    "    page_num = 0\n",
    "\n",
    "    title = []\n",
    "    authors = []\n",
    "    date = []\n",
    "    paper_url = []\n",
    "\n",
    "    while page_num < num_pages:\n",
    "        if page_num == 0:\n",
    "            page_url = search_url\n",
    "\n",
    "        else:\n",
    "            page_url = search_url + '?page=' + str(page_num)\n",
    "\n",
    "        for container in read_search_page(page_url):\n",
    "            result_url = 'https://www.' + site + '.org' + container.a['href']  \n",
    "            paper_url.append(result_url)\n",
    "            title.append(container.a.span.text)\n",
    "            result_authors = container.findAll('span', {'class':'highwire-citation-authors'})[0].text\n",
    "            authors.append(result_authors)\n",
    "\n",
    "        page_num += 1\n",
    "\n",
    "    abstract = []\n",
    "    introduction = []\n",
    "    methods = []\n",
    "    results = []\n",
    "    discussion = []\n",
    "    \n",
    "    num_urls = len(paper_url)\n",
    "    progress1 = ttk.Progressbar(topleftframe, orient = HORIZONTAL, length=100, mode = 'determinate')\n",
    "    progress1.grid(row = 1, column = 8, sticky = E, pady = 2) \n",
    "    i = 1\n",
    "    \n",
    "    for url in tqdm(paper_url):\n",
    "        progress1[\"value\"] = i*100/num_urls\n",
    "        l3_string.set(f'Scraping paper {i} of {num_urls}')\n",
    "        root.update()\n",
    "        i += 1   \n",
    "        \n",
    "        abst, intr, rslt, methd, disc, dt = read_article_page(url)\n",
    "        abstract += [abst]\n",
    "        introduction += [intr]\n",
    "        results += [rslt]\n",
    "        methods += [methd]\n",
    "        discussion += [disc]\n",
    "        date += [dt]          \n",
    "        \n",
    "    progress1.grid_remove()\n",
    "    df = pd.DataFrame(list(zip(title, authors, date, paper_url, abstract, introduction, results, methods, discussion)),\n",
    "                          columns = ['Title','Authors','Date','Paper Url','Abstract', 'Introduction', 'Results', 'Methods', 'Discussion'])\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For processing articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the most average sentence (or sentences) in a block of text along with an averaged out embedding for the text\n",
    "def return_average_sentences(text, num_returned=1):\n",
    "    returned = num_returned\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)  \n",
    "    embeds = model.encode(sentences)\n",
    "    avg = sum(embeds) / len(embeds)    \n",
    "    cosim = cosine_similarity([avg], embeds)   \n",
    "    average_sentences = [sentences[idx] for idx in cosim.argsort()[0][-returned:]]\n",
    "\n",
    "    return average_sentences, avg\n",
    "\n",
    "# Clusters preprints based their most average sentence embeddings\n",
    "def cluster_papers(dataframe):\n",
    "    X = dataframe['Average Embedding'].tolist()\n",
    "    max_clusters = 200 # just to not slow down the processing too much, number is a bit arbitrary\n",
    "    if max_clusters > int(len(X)):\n",
    "        max_clusters = int(len(X))\n",
    "    k_rng = range(1, max_clusters)\n",
    "    sse = []\n",
    "    for k in k_rng:\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km.fit(X)\n",
    "        sse.append(km.inertia_)\n",
    "\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Sum of squared error')\n",
    "    plt.plot(k_rng, sse)\n",
    "\n",
    "    # Automatic quick elbow method; just picks the point with the longest distance away from the line between the two most outer points\n",
    "    x1 = k_rng[0]\n",
    "    y1 = sse[0]\n",
    "    x2 = k_rng[-1]\n",
    "    y2 = sse[-1]\n",
    "\n",
    "    x = [x1, x2]\n",
    "    y = [y1, y2]\n",
    "    coefficients = np.polyfit(x, y, 1)\n",
    "    polynom = np.poly1d(coefficients)\n",
    "    x = np.array(k_rng)\n",
    "    y = polynom(x)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "    p1 = np.array([x1, y1])\n",
    "    p2 = np.array([x2, y2])\n",
    "    distance = []\n",
    "\n",
    "    candidate_points = []\n",
    "\n",
    "    for x, y in zip(k_rng, sse):\n",
    "        p3 = [x, y]\n",
    "        d = np.linalg.norm(np.cross(p2-p1, p1-p3))/np.linalg.norm(p2-p1)\n",
    "        candidate_points.append(p3)\n",
    "        distance.append(d)\n",
    "\n",
    "    num_clusters = np.argsort(distance)[-1]\n",
    "    point = candidate_points[num_clusters]\n",
    "\n",
    "    print('Num clusters: ' + str(num_clusters))\n",
    "    plt.plot(point[0], point[1], 'rp', markersize=10)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Adds the clusters as well as cluster centers to the dataframe\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    y_predicted = km.fit_predict(X)\n",
    "    centers = km.cluster_centers_\n",
    "\n",
    "    dataframe['Cluster'] = y_predicted\n",
    "    dataframe['Centers'] = dataframe['Cluster'].apply(lambda x: centers[x])\n",
    "\n",
    "    dataframe = dataframe.sort_values('Cluster')\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Returns the average sentences and embeddings for each article\n",
    "def process_articles(dataframe):\n",
    "    abbreviated_texts = []\n",
    "    average_sentences = []\n",
    "    average_embeddings = []\n",
    "    keywords = []\n",
    "\n",
    "    num_articles = dataframe.shape[0]\n",
    "    progress2 = ttk.Progressbar(topleftframe, orient = HORIZONTAL, length=100, mode = 'determinate')\n",
    "    progress2.grid(row = 1, column = 8, sticky = E, pady = 2) \n",
    "    progress_idx = 1\n",
    "        \n",
    "    with tqdm(total=num_articles) as pbar: \n",
    "        for row in dataframe.itertuples(index=True):\n",
    "            pbar.update(1)\n",
    "            text = row[5:10]\n",
    "            merged_text = []\n",
    "            abbreviated_article = []\n",
    "            progress2[\"value\"] = progress_idx*100/num_articles\n",
    "            l3_string.set(f'Processing paper {progress_idx} of {num_articles}')\n",
    "            root.update()\n",
    "            progress_idx += 1\n",
    "\n",
    "            for i, section in enumerate(text):\n",
    "                section_avg_sentences = []\n",
    "                for j, paragraph in enumerate(section):\n",
    "                    if paragraph:\n",
    "                        paragraph = re.sub(' \\([^)]*\\)', '', paragraph) \n",
    "                        paragraph = re.sub(' \\[.*?\\]', '', paragraph) #for removing brackets and parentheses (and text in between)\n",
    "                        para_avg_sentence, _ = return_average_sentences(paragraph)                \n",
    "                        merged_text.append(paragraph)                \n",
    "                        section_avg_sentences.append(para_avg_sentence[0])\n",
    "                abbreviated_article.append(section_avg_sentences)\n",
    "            abbreviated_texts.append(abbreviated_article)\n",
    "            merged_text = ' '.join(merged_text)\n",
    "            keywords_article = get_keywords([merged_text])\n",
    "            keywords.append(keywords_article)\n",
    "            sentence, average_embed = return_average_sentences(merged_text)\n",
    "            average_sentences.append(sentence) \n",
    "            average_embeddings.append(average_embed)\n",
    "            \n",
    "            \n",
    "    dataframe['Average Text'] = abbreviated_texts\n",
    "    dataframe['Average Sentence'] = average_sentences\n",
    "    dataframe['Average Embedding'] = average_embeddings\n",
    "    dataframe['Keywords'] = keywords\n",
    "    \n",
    "    progress2.grid_remove()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Returns keywords from a text; largely from: https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea\n",
    "def get_keywords(text):\n",
    "    n_gram_range = (1, 1)\n",
    "    returned = 5\n",
    "        \n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words='english').fit(text)\n",
    "    words = count.get_feature_names()\n",
    "    gen_embedding = model.encode(text)\n",
    "    word_embeddings = model.encode(words)\n",
    "    cosim = cosine_similarity(gen_embedding, word_embeddings)\n",
    "    keywords = [words[idx] for idx in cosim.argsort()[0][-returned:]]\n",
    "    return keywords\n",
    "\n",
    "# Returns keywords for each cluster\n",
    "def get_cluster_keywords(dataframe):\n",
    "    cluster_keywords = []\n",
    "    clusters = dataframe['Cluster'].unique()\n",
    "    for cluster in clusters:\n",
    "        cluster_text = []\n",
    "        for sentence in dataframe['Average Sentence'][dataframe.Cluster==cluster]:\n",
    "            cluster_text.append(sentence[0])\n",
    "        keywords = get_keywords(cluster_text)\n",
    "        cluster_keywords.append(keywords)\n",
    "    dataframe['Cluster Keywords'] = dataframe['Cluster'].apply(lambda x: cluster_keywords[x])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Sorts the dataframe by proximity to a center cluster, and then by cluster\n",
    "def cluster_center_sort(dataframe):\n",
    "    center_proximity = []\n",
    "    clusters = dataframe['Cluster'].unique()\n",
    "    for cluster in clusters:\n",
    "        cluster_embeddings = []\n",
    "        cluster_center = (dataframe['Centers'][dataframe.Cluster==cluster].tolist())[0]\n",
    "        for embedding in dataframe['Average Embedding'][dataframe.Cluster==cluster]:\n",
    "            cluster_embeddings.append(embedding)\n",
    "        order = range(0, len(embedding-1))\n",
    "        cosim = cosine_similarity([cluster_center], cluster_embeddings)\n",
    "        center_proximity = np.concatenate((center_proximity, *cosim))\n",
    "    dataframe['Center Proximity'] = center_proximity\n",
    "    \n",
    "    dataframe = dataframe.sort_values(['Center Proximity'], ascending=False)\n",
    "    dataframe = dataframe.sort_values(['Cluster'])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets up text scraping and processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rxiv_search(search_url, filename, max_results):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ################################################################\n",
    "    max_pages = math.ceil(max_results/10)\n",
    "    \n",
    "    print('Scraping papers')   \n",
    "    df = get_papers(search_url, max_pages)    \n",
    "    df.to_excel('scraped_texts/'+filename+'.xlsx', index=False)\n",
    "    \n",
    "    ##############################################################\n",
    "    print('Processing papers')\n",
    "\n",
    "    df = process_articles(df)\n",
    "    df = cluster_papers(df)  \n",
    "    df = get_cluster_keywords(df)    \n",
    "    df = cluster_center_sort(df)\n",
    "    \n",
    "    if not os.path.exists('processed_texts/'):\n",
    "        os.makedirs('processed_texts/')\n",
    "    \n",
    "    #df.to_excel('processed_texts/'+filename+'.xlsx', index=False)\n",
    "    df.to_pickle('processed_texts/'+filename+'.df')\n",
    "    \n",
    "    end_time = time.time() \n",
    "    duration = (end_time-start_time)\n",
    "    duration = round(duration, 2)\n",
    "    time_unit = ' seconds'\n",
    "    if duration > 60:\n",
    "        time_unit = ' minutes'\n",
    "        duration = duration/60 \n",
    "        \n",
    "    duration = round(duration, 1)\n",
    "    duration = str(duration) + time_unit\n",
    "    num_results = len(df)\n",
    "\n",
    "    print('Retrieval complete. Total time: ' + duration)\n",
    "       \n",
    "    return df, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the GUI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wraps text\n",
    "def wrap(string, length=8):\n",
    "    return '\\n'.join(textwrap.wrap(string, length))\n",
    "\n",
    "# Returns a shortened list of author names\n",
    "def abbrev_names(names):\n",
    "    abbrvnames = ''\n",
    "    if len(names) == 3:\n",
    "        abbrvnames = abbrev_name(names[0]) + ', ' + abbrev_name(names[1]) + ', and ' + abbrev_name(names[2])\n",
    "    if len(names) == 2:\n",
    "        abbrvnames = abbrev_name(names[0]) + ', and ' + abbrev_name(names[1])\n",
    "    if len(names) == 1:\n",
    "        abbrvnames = abbrev_name(names[0])\n",
    "    else:\n",
    "        abbrvnames = abbrev_name(names[0]) + ' et al.'\n",
    "    return abbrvnames\n",
    "\n",
    "def abbrev_name(name):\n",
    "    abbrvname = name.split(' ')\n",
    "    surname = abbrvname[-1]\n",
    "    givennames = ' '.join(abbrvname[:-1])\n",
    "    abbrvname = ''.join(re.findall('[A-Z]', givennames)) + ' ' + surname\n",
    "    return abbrvname\n",
    "\n",
    "# Opens a browser link for the preprint\n",
    "def on_web_click(event):\n",
    "    url = url_string.get()\n",
    "    webbrowser.open(url)\n",
    "\n",
    "# Puts summarized (shortened) text for a preprint on the right of the window\n",
    "def on_double_click(event):\n",
    "    item_id = event.widget.focus()\n",
    "    item = event.widget.item(item_id)\n",
    "    values=item['values']\n",
    "    title=' '.join((values[3]).split('\\n'))\n",
    "    authors=values[1]\n",
    "    date=' '.join((values[2]).split('\\n'))\n",
    "    url=values[4]\n",
    "    keywords=values[5]\n",
    "    \n",
    "    titletext.config(state=NORMAL)\n",
    "    bodytext.config(state=NORMAL)\n",
    "    titletext.delete('1.0', END)\n",
    "    bodytext.delete('1.0', END)\n",
    "    \n",
    "    a, i, r, m, d = values[6], values[7], values[8], values[9], values[10]\n",
    "    \n",
    "    if len(a) > 1:\n",
    "        bodytext.insert(END, 'ABSTRACT\\n', 'heading')\n",
    "        bodytext.insert(END, a+'\\n\\n', 'body')\n",
    "        \n",
    "    if len(i) > 1:\n",
    "        bodytext.insert(END, 'INTRODUCTION\\n', 'heading')\n",
    "        bodytext.insert(END, i+'\\n\\n', 'body')\n",
    "        \n",
    "    if len(d) > 1:\n",
    "        bodytext.insert(END, 'DISCUSSION\\n', 'heading')\n",
    "        bodytext.insert(END, d+'\\n\\n', 'body')\n",
    "    \n",
    "    if len(r) > 1:\n",
    "        bodytext.insert(END, 'RESULTS\\n', 'heading')\n",
    "        bodytext.insert(END, r+'\\n\\n', 'body')\n",
    "    \n",
    "    if len(m) > 1:\n",
    "        bodytext.insert(END, 'METHODS\\n', 'heading')\n",
    "        bodytext.insert(END, m+'\\n\\n', 'body')\n",
    "        \n",
    "    \n",
    "    title_text = title + '\\n' + authors + '\\n' + date# + '\\n' + keywords\n",
    "    titletext.insert(END, title_text, 'title')\n",
    "    \n",
    "    titletext.config(state=DISABLED)\n",
    "    bodytext.config(state=DISABLED)\n",
    "    \n",
    "    url_string.set(url)\n",
    "    \n",
    "# Loads in a dataframe from an older search    \n",
    "def oldsearches(event):\n",
    "    file =  'processed_texts/'+searches.get()\n",
    "    currdf = pd.read_pickle(file+'.df')\n",
    "    remove_data()\n",
    "    add_data(currdf)\n",
    "    \n",
    "# Sets off the rxiv search\n",
    "def search():\n",
    "    max_results = maxresults.get()\n",
    "    max_results = int(''.join(re.findall('\\d', max_results)))\n",
    "    \n",
    "    maxresults.grid(row = 1, column = 5, sticky = E, pady = 2) \n",
    "    collectbtn.grid(row = 1, column = 6, sticky = E, pady = 2)\n",
    "    maxresults.grid_forget()\n",
    "    collectbtn.grid_forget()\n",
    "    \n",
    "    search_term = searchfield.get()\n",
    "    filename = search_term + ' ' + sites.get()\n",
    "    search_term = search_term.split(' ')\n",
    "    search_term = '%252B'.join(search_term)\n",
    "    site = str(sites.get())\n",
    "    first_part = 'https://www.' + site + '.org/search/'\n",
    "    search_url = first_part + search_term + '%20numresults%3A10%20sort%3Apublication-date%20direction%3Adescending' \n",
    "    newdf, duration = rxiv_search(search_url, filename, max_results)\n",
    "\n",
    "    remove_data()\n",
    "    add_data(newdf) \n",
    "    l3_string.set(('Retrieval complete. Total time: {}. Retrieved {} articles').format(duration, len(newdf)))\n",
    "\n",
    "# Puts data into the treeview\n",
    "def add_data(df):\n",
    "    clusters = (df['Cluster'].unique())\n",
    "    l4_string.set(str(len(df)) + ' articles found')\n",
    "    for cluster in clusters:\n",
    "        keywords = ', '.join(df['Cluster Keywords'][df.Cluster==cluster].tolist()[0])\n",
    "        tree.insert(parent='', index='end', iid=cluster+1, text=str(cluster+1), values=(keywords,'','','',''), tags=('parent',))\n",
    "    \n",
    "    for idx, cluster, title, url, authors, date, avg_sentence, keywords, text in zip(df.index, df.Cluster, df.Title, df['Paper Url'], df.Authors, df.Date, df['Average Sentence'], df['Keywords'],df['Average Text']):\n",
    "        paper_url = url\n",
    "        authors = abbrev_names(authors.split(', '))\n",
    "        tree.insert(parent=(cluster+1), index='end', iid=idx+len(clusters)+1, text='', values=(wrap(avg_sentence[0], 85), authors, wrap(date,10), wrap(title,40), url, keywords, \n",
    "                                                                                               '\\u2022' + '\\n\\u2022'.join(text[0]), \n",
    "                                                                                               '\\u2022' + '\\n\\u2022'.join(text[1]), \n",
    "                                                                                               '\\u2022' + '\\n\\u2022'.join(text[2]), \n",
    "                                                                                               '\\u2022' + '\\n\\u2022'.join(text[3]), \n",
    "                                                                                               '\\u2022' + '\\n\\u2022'.join(text[4])))\n",
    "# Clears data from the treeview\n",
    "def remove_data():\n",
    "    tree.delete(*tree.get_children())\n",
    "    l3_string.set('')\n",
    "    l4_string.set('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "root.title('Preprint summarizer')\n",
    "root.geometry('1530x900')\n",
    "root.grid_propagate(False)\n",
    "\n",
    "old_searches = []\n",
    "for file in os.listdir('processed_texts/'):\n",
    "    if file.endswith('.df'):\n",
    "        old_searches.append(file[:-3])\n",
    "        \n",
    "l3_string = StringVar(value='')\n",
    "l4_string = StringVar(value='')\n",
    "url_string = StringVar(value='')\n",
    "\n",
    "#################################################################################\n",
    "# Setting up the main frames:\n",
    "mainframe = Frame(root)\n",
    "mainframe.pack(expand=True, fill=BOTH, padx=25, pady=10, anchor=NW)\n",
    "\n",
    "topframe = Frame(mainframe)\n",
    "topframe.pack(side=TOP, anchor=NW, expand=False, fill=X)\n",
    "\n",
    "botframe = Frame(mainframe)\n",
    "botframe.pack(side=BOTTOM, anchor=NW, expand=True, fill=Y, pady=2)\n",
    "\n",
    "topleftframe = Frame(topframe)\n",
    "topleftframe.pack(side=LEFT, anchor=NW, expand=False)\n",
    "\n",
    "botleftframe = Frame(botframe)\n",
    "botleftframe.pack(side=LEFT, anchor=NW, expand=True, fill=Y)\n",
    "\n",
    "botrightframe = Frame(botframe)\n",
    "botrightframe.pack(side=RIGHT, anchor=NW, expand=True, fill=Y, padx=15)\n",
    "##################################################################################\n",
    "\n",
    "# Labels, buttons, and search boxes:\n",
    "l0 = Label(topleftframe, text = \"Website:\")\n",
    "l0.grid(row = 1, column = 1, sticky = W)\n",
    "\n",
    "sites = ttk.Combobox(topleftframe, height=1, width=35)\n",
    "sites['values'] = ['biorxiv','medrxiv']\n",
    "sites.current(0)\n",
    "sites.grid(row = 1, column = 2, sticky = W, pady=2, padx=2)\n",
    "\n",
    "l1 = Label(topleftframe, text = \"Search:\")\n",
    "l1.grid(row = 1, column = 3, sticky = W)\n",
    "\n",
    "searchfield = Entry(topleftframe, width=38)\n",
    "searchfield.grid(row = 1, column = 3, sticky = W, padx=2)\n",
    "\n",
    "searchbtn = Button(topleftframe, text='Search', command=get_num_results)\n",
    "searchbtn.grid(row = 1, column = 4, sticky = W)\n",
    "\n",
    "collectbtn = Button(topleftframe, text='Retrieve', command=search)\n",
    "maxresults = ttk.Combobox(topleftframe, height=1, width=15)\n",
    "\n",
    "l2 = Label(topleftframe, text = \"Past searches:\")\n",
    "l2.grid(row = 2, column = 1, sticky = W)\n",
    "\n",
    "searches = ttk.Combobox(topleftframe, height=1, width=35)\n",
    "searches['values'] = old_searches\n",
    "searches.bind(\"<<ComboboxSelected>>\", oldsearches)\n",
    "searches.grid(row = 2, column = 2, sticky = W, pady=2)\n",
    "\n",
    "l3 = Label(topleftframe, textvariable=l3_string)\n",
    "l3.grid(row = 1, column = 5, sticky = W, padx=5, pady=2)\n",
    "\n",
    "l4 = Label(topleftframe, textvariable=l4_string)\n",
    "l4.grid(row = 2, column = 3, sticky = W, padx=5, pady=2)\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# Treeview:\n",
    "s = ttk.Style()\n",
    "s.configure('Treeview',rowheight=78)\n",
    "s.configure('Treeview.Heading', font=(None, 10))\n",
    "s.map('Treeview', background=[('selected','blue')])\n",
    "\n",
    "tree = ttk.Treeview(botleftframe)\n",
    "tree.bind(\"<Double-Button-1>\", on_double_click)\n",
    "\n",
    "tree.tag_configure('parent', background='white', font=(None, 10, 'italic'))\n",
    "tree.tag_configure('oddrow', background='white')\n",
    "\n",
    "tree['columns'] = ('Average Sentence', 'Authors', 'Date', 'Title')\n",
    "\n",
    "tree.column('#0', width=50, minwidth=25)\n",
    "tree.column('Average Sentence', anchor=W, width=480)\n",
    "tree.column('Authors', anchor=W, width=100)\n",
    "tree.column('Date', anchor=W, width=70)\n",
    "tree.column('Title', anchor=W, width=250)\n",
    "\n",
    "tree.heading('#0', text='Cluster', anchor=W)\n",
    "tree.heading('Average Sentence', text='Average Sentence', anchor=W)\n",
    "tree.heading('Authors', text='Authors', anchor=W)\n",
    "tree.heading('Date', text='Date', anchor=CENTER)\n",
    "tree.heading('Title', text='Title', anchor=W)\n",
    "\n",
    "tree.pack(side=LEFT, expand=True, fill=BOTH, anchor=NW, pady=2)\n",
    "\n",
    "textcanvas = Canvas(botrightframe, background='white', relief='groove', bd=1)\n",
    "textcanvas.pack(expand=True, fill=BOTH, anchor=NW)\n",
    "\n",
    "titletext= Text(textcanvas, bd=0, height=6)\n",
    "titletext.tag_configure('title', font=(None, 11, 'italic'))\n",
    "titletext.bind('<Double-Button-1>', on_web_click)\n",
    "titletext.pack(side=TOP, expand=False, fill=X, anchor=NW, padx=10, pady=5)\n",
    "\n",
    "bodytext = Text(textcanvas, bd=0)\n",
    "bodytext.tag_configure('body', font=(None, 10))\n",
    "bodytext.tag_configure('heading', font=(None, 11))\n",
    "bodytext.pack(side=LEFT, expand=True, fill=BOTH, anchor=NW, padx=10, pady=5)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
